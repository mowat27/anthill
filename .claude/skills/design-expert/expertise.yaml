# Antkeeper Framework - Design Expertise
# Mental model for the antkeeper workflow engine
# Last validated: 2026-02-11

overview:
  name: antkeeper
  purpose: >
    Lightweight Python workflow engine. Users define handlers (workflow steps)
    via a decorator-based App, wire them to a Channel (I/O boundary), and
    execute through a Runner. Designed for composable, testable pipelines.
    Supports CLI, HTTP API, and Slack-triggered execution.
  python: ">=3.12"
  license: MIT
  build: uv (uv_build backend)
  entry_point: "antkeeper.cli:main"
  dependencies:
    - python-dotenv (.env file loading)
    - httpx (Slack API calls, HTTP client — core dependency)
  optional_dependencies:
    server: ["fastapi", "uvicorn[standard]"]
    all: ["antkeeper[server]"]
  dev_dependencies:
    - pytest (test)
    - fastapi (web framework)
    - uvicorn[standard] (ASGI server)
    - ruff (lint)
    - ty (type-check)
  justfile_default: "check -> ruff, ty, test"
  justfile_recipes:
    - "sdlc prompt model='opus' — runs sdlc workflow, auto-detects --prompt vs --prompt-file"
    - "sdlc_iso prompt model='opus' — runs sdlc_iso workflow (worktree-isolated), same prompt detection"
    - "server — runs antkeeper server with default settings"
    - "check_api host port — curl POST to /webhook for healthcheck"

architecture:
  core_pattern: >
    Reducer pattern. Each workflow step receives (runner, state) and returns
    a new State dict. run_workflow folds state through a list of steps.
    State is ephemeral and never shared between workflow runs — this ensures
    thread safety (one App serves multiple concurrent workflows) and makes
    reasoning straightforward.
  layer_rules:
    - "Outer layers (channels, CLI) depend on inner layers (core). Never the reverse."
    - "Core must be truly generic — no channel-specific or domain-specific concepts."
    - "Channels own runtime type/value checks to prevent bad data reaching core."
    - "Errors generally propagate up to handlers: missing keys, failed connections, naming mistakes."
    - "Edge validation (e.g. missing CLI params) can be handled and reported only when a stack trace adds no value."
  layers:
    - name: core
      path: src/antkeeper/core/
      responsibility: >
        Framework kernel. Contains domain types, the App handler registry,
        the Runner execution engine, and the run_workflow composition helper.
      modules:
        - domain.py
        - app.py
        - runner.py

    - name: channels
      path: src/antkeeper/channels/
      responsibility: >
        I/O boundary adapters. Each channel satisfies the Channel protocol
        and owns how progress/errors are reported and what initial state is
        supplied. Channels are the seam where tests swap in doubles.
      modules:
        - cli.py    # CliChannel for command-line execution
        - api.py    # ApiChannel for HTTP API execution
        - slack.py  # SlackChannel for Slack thread-based execution

    - name: llm
      path: src/antkeeper/llm/
      responsibility: >
        LLM agent abstraction layer. Defines the Agent protocol for LLM
        interactions and provides concrete implementations (ClaudeCodeAgent).
        Handlers use agents to delegate prompts to external LLM tooling.
      modules:
        - __init__.py   # Agent protocol
        - errors.py     # AgentExecutionError
        - claude_code.py  # ClaudeCodeAgent (subprocess-based)

    - name: git
      path: src/antkeeper/git/
      responsibility: >
        Git integration layer. Provides worktree management for isolated
        workflow execution. Handlers use git worktrees to run SDLC steps
        in separate working directories without affecting the main checkout.
      modules:
        - __init__.py     # Re-exports Worktree, WorktreeError, git_worktree
        - worktrees.py    # Worktree class, WorktreeError, git_worktree context manager

    - name: helpers
      path: src/antkeeper/helpers/
      responsibility: >
        Shared utility functions. Currently contains JSON extraction for
        parsing LLM responses that embed JSON in markdown or prose.
      modules:
        - __init__.py  # Re-exports extract_json via __all__
        - json.py  # extract_json — finds first {…} in text, parses as JSON

    - name: http
      path: src/antkeeper/http/
      responsibility: >
        HTTP endpoint logic extracted from server.py. __init__.py provides
        run_workflow_background shared helper. webhook.py exports handle_webhook
        async function. slack_events.py exports SlackEventProcessor class.
        server.py defines routes inline and delegates to these modules.
      modules:
        - __init__.py      # run_workflow_background — shared background task helper
        - webhook.py       # handle_webhook async function, WebhookRequest/Response models
        - slack_events.py  # SlackEventProcessor class with debounce timer, pending store

    - name: cli
      path: src/antkeeper/cli.py
      responsibility: >
        Argparse-based CLI entry point. Loads a user-supplied agents file
        (Python module exporting `app`), builds a CliChannel, and runs
        the workflow via Runner. Also provides server subcommand that starts
        uvicorn for HTTP API access.

    - name: server
      path: src/antkeeper/server.py
      responsibility: >
        FastAPI server orchestrator. Loads .env via python-dotenv, loads
        antkeeper App from agents file, creates FastAPI instance, defines
        routes inline (@api.post), and delegates logic to http/webhook.py
        (handle_webhook) and http/slack_events.py (SlackEventProcessor).
        Module-level app = create_app() for uvicorn import.

    - name: package_init
      path: src/antkeeper/__init__.py
      responsibility: >
        Package-level re-exports. Exports all major public types via __all__:
        App, Runner, run_workflow, State, Channel, WorkflowFailedError,
        CliChannel, ApiChannel, SlackChannel, Worktree, git_worktree.
        Allows users to import from antkeeper directly.

    - name: package_main
      path: src/antkeeper/__main__.py
      responsibility: >
        Enables `python -m antkeeper` execution. Delegates to cli.main().

  logging: >
    File-based per-run logging via Python stdlib logging. Runner creates a
    per-run logger (antkeeper.run.<id>) writing to <app.log_dir>/<timestamp>-<id>.log.
    Module-level loggers exist in cli.py, channels/cli.py, llm/claude_code.py,
    and git/worktrees.py for debug tracing. No console logging by default —
    all framework logs go to files.

  state_persistence: >
    Automatic JSON-based state snapshots. Runner writes state to
    <app.state_dir>/<timestamp>-<id>.json. State is persisted at three points:
    (1) before workflow execution (initial state with run_id/workflow_name),
    (2) after each run_workflow step completes, and (3) after workflow execution
    completes (final state). The state file and log file share the same
    <timestamp>-<id> base name for easy correlation. State files use formatted
    JSON (indent=2) for human readability.

domain_types:
  WorkflowFailedError:
    file: src/antkeeper/core/domain.py
    definition: "class WorkflowFailedError(Exception)"
    notes: >
      Raised by Runner.fail() to signal workflow failure. Replaces the previous
      SystemExit behavior. CLI catches this and exits with code 1. API server
      catches it silently (already logged by Runner).

  State:
    file: src/antkeeper/core/domain.py
    definition: "type State = dict[str, Any]"
    notes: >
      Type alias. All workflow state is a flat string-keyed dict. Handlers
      receive and return State; Runner injects run_id and workflow_name.

  Channel:
    file: src/antkeeper/core/domain.py
    definition: Protocol
    attributes:
      - "type: str"
      - "workflow_name: str"
      - "initial_state: State"
    methods:
      - "report_progress(run_id: str, message: str, **opts: Any) -> None"
      - "report_error(run_id: str, message: str) -> None"
    notes: >
      Structural (duck-typed) protocol. Any object with these attrs/methods
      qualifies. This is the primary extension point for new I/O adapters.

core_objects:
  App:
    file: src/antkeeper/core/app.py
    responsibility: Handler registry keyed by function name.
    construction: "App(log_dir: str = 'agents/logs/', worktree_dir: str = 'trees/', state_dir: str = '.antkeeper/state/')"
    attributes:
      - "handlers: dict  # name -> fn mapping"
      - "log_dir: str  # directory for per-run log files"
      - "worktree_dir: str  # directory for git worktrees"
      - "state_dir: str  # directory for per-run state JSON files"
    interface:
      - "handler(fn) -> wrapper  # decorator that registers fn by fn.__name__"
      - "get_handler(name) -> Callable[[Runner, State], State | NoReturn]"
    design_constraint: >
      get_handler raises ValueError for unknown names. The decorator stores
      the original fn (not the wrapper) in self.handlers. log_dir is passed
      to Runner for per-run file logging. worktree_dir is used by handlers
      (e.g. sdlc_iso) to locate worktree base directories. state_dir is
      passed to Runner for per-run state JSON persistence.

  Runner:
    file: src/antkeeper/core/runner.py
    responsibility: >
      Execution engine. Binds an App + Channel, generates a run id, sets up
      per-run file logging, and drives the workflow lifecycle.
    construction: "Runner(app: App, channel: Channel)"
    attributes:
      - "id: str  # uuid4 hex[:8]"
      - "channel: Channel"
      - "app: App"
      - "logger: logging.Logger  # per-run file logger (antkeeper.run.<id>)"
      - "_state_path: str  # absolute path to per-run state JSON file"
    interface:
      - "run() -> State  # merges channel.initial_state with run_id/workflow_name, persists state before/after workflow, calls workflow"
      - "_persist_state(state: State) -> None  # writes state dict to _state_path as formatted JSON"
      - "workflow_name: property  # delegates to channel.workflow_name"
      - "workflow: property  # resolves handler via app.get_handler(workflow_name)"
      - "report_progress(message) -> None  # logs then delegates to channel"
      - "report_error(message) -> None  # logs then delegates to channel"
      - "fail(message) -> NoReturn  # logs, raises WorkflowFailedError"
    design_constraint: >
      Runner.__init__ creates log dir (app.log_dir) and state dir (app.state_dir),
      sets up a FileHandler writing to <log_dir>/<YYYYMMDDHHMMSS>-<id>.log and
      a state file at <state_dir>/<YYYYMMDDHHMMSS>-<id>.json. Logger propagate=False
      to avoid console output. run() persists state before and after workflow
      execution via _persist_state(). run_workflow() also calls _persist_state()
      after each step completes, enabling mid-workflow state recovery. run() wraps
      workflow call in try/except to log errors before re-raising. fail() raises
      WorkflowFailedError instead of calling exit(1) — this allows CLI and API
      to handle failures differently (CLI exits, API continues serving).

  run_workflow:
    file: src/antkeeper/core/app.py
    signature: "run_workflow(runner: Runner, state: State, steps: list[Callable]) -> State"
    responsibility: >
      Sequential composition helper. Folds state through a list of handler
      callables. Logs step names and completion via runner.logger. Persists
      state to disk (via runner._persist_state) after each step completes,
      enabling mid-workflow state inspection and recovery.

git_objects:
  WorktreeError:
    file: src/antkeeper/git/worktrees.py
    definition: "class WorktreeError(Exception)"
    notes: >
      Raised when a git worktree operation fails (create, remove, or
      entering a non-existent worktree).

  Worktree:
    file: src/antkeeper/git/worktrees.py
    responsibility: >
      Represents a git worktree on disk. Wraps git worktree subprocess
      operations for creating and removing worktrees.
    construction: "Worktree(base_dir: str, name: str)"
    attributes:
      - "base_dir: str  # absolute path to the directory containing worktrees"
      - "name: str  # worktree subdirectory name"
      - "path: str  # absolute path (realpath of base_dir/name)"
    interface:
      - "exists: property -> bool  # checks os.path.isdir(self.path)"
      - "create(branch: str | None = None) -> None  # git worktree add, creates base_dir if needed"
      - "remove() -> None  # git worktree remove"
    design_constraint: >
      Paths are always resolved via os.path.realpath for consistency.
      create() uses os.makedirs(exist_ok=True) for the base directory.
      Both create() and remove() raise WorktreeError on non-zero git exit.
      Logging via module-level logger (antkeeper.git.worktrees).

  git_worktree:
    file: src/antkeeper/git/worktrees.py
    signature: "git_worktree(worktree, *, create=False, branch=None, remove=False) -> Generator[Worktree]"
    responsibility: >
      Context manager that enters a git worktree directory. Guarantees cwd
      restoration via try/finally. Optionally creates the worktree on entry
      and removes it on exit.
    behavior:
      entry: >
        If create=True, calls worktree.create(branch=branch). If create=False
        and worktree does not exist, raises WorktreeError. Saves current cwd
        and chdir to worktree.path.
      exit: >
        Restores original cwd (always, even on exception). If remove=True,
        calls worktree.remove() after restoring cwd.
      yields: "The Worktree instance that was entered."

llm_types:
  Agent:
    file: src/antkeeper/llm/__init__.py
    definition: Protocol
    methods:
      - "prompt(prompt: str) -> str"
    notes: >
      Structural protocol for LLM agents. Any object with a prompt() method
      qualifies. This is the extension point for adding new LLM backends.

  AgentExecutionError:
    file: src/antkeeper/llm/errors.py
    definition: "class AgentExecutionError(Exception)"
    notes: >
      Raised when an agent fails to execute a prompt (non-zero exit,
      missing binary, etc.).

  ClaudeCodeAgent:
    file: src/antkeeper/llm/claude_code.py
    satisfies: Agent protocol
    construction: "ClaudeCodeAgent(model: str | None = None)"
    behavior:
      prompt: >
        Runs `claude -p <prompt>` via subprocess. If model is set,
        inserts `--model <model>` flag. Returns stdout on success.
        Logs prompt submission, response, and errors via module logger.
      errors:
        - "Non-zero exit -> AgentExecutionError with stderr"
        - "Missing binary -> AgentExecutionError('claude binary not found')"

channel_implementations:
  CliChannel:
    file: src/antkeeper/channels/cli.py
    satisfies: Channel protocol
    construction: "CliChannel(workflow_name: str, initial_state: dict[str,str] | None = None)"
    behavior:
      type: '"cli"'
      initial_state: "Copies input dict or defaults to {}"
      report_progress: "Prints formatted [workflow_name, run_id] message to stdout with flush=True"
      report_error: "Delegates to report_progress with file=sys.stderr"

  ApiChannel:
    file: src/antkeeper/channels/api.py
    satisfies: Channel protocol
    construction: "ApiChannel(workflow_name: str, initial_state: dict[str,str] | None = None)"
    behavior:
      type: '"api"'
      initial_state: "Copies input dict or defaults to {}"
      report_progress: "Prints formatted [workflow_name, run_id] message to stdout with flush=True"
      report_error: "Delegates to report_progress with file=sys.stderr"
    notes: >
      Identical to CliChannel except type="api". Both use flush=True.
      Designed for FastAPI server where output goes to Uvicorn logs.

  SlackChannel:
    file: src/antkeeper/channels/slack.py
    satisfies: Channel protocol
    construction: "SlackChannel(workflow_name: str, initial_state: State | None = None, *, slack_token: str, channel_id: str, thread_ts: str)"
    behavior:
      type: '"slack"'
      initial_state: "Copies input dict or defaults to {}"
      report_progress: "Posts [workflow_name, run_id] message to Slack thread via httpx sync POST"
      report_error: "Posts [workflow_name, run_id] [ERROR] message to Slack thread"
    design_constraint: >
      Uses sync httpx.Client (not async) because handlers run in threadpool
      via asyncio.to_thread. HTTP errors are caught and logged, never
      propagated — Slack failures must not crash the workflow. Thread context
      (channel_id, thread_ts) captured at construction from the original
      @mention event.

cli_entry_point:
  file: src/antkeeper/cli.py
  shared_functions:
    - "load_app(path: str) -> App  # dynamically imports module, extracts app attr"
    - "parse_state_pairs(pairs: list[str]) -> dict[str, str]  # splits key=val strings"
  subcommands:
    run:
      args:
        - "--agents-file (default: handlers.py) — path to Python module exporting `app`"
        - "--initial-state key=val (repeatable) — seed state pairs"
        - "--prompt — user prompt string, stored as state['prompt'] (mutually exclusive with --prompt-file)"
        - "--prompt-file — path to file whose contents become state['prompt'] (mutually exclusive with --prompt)"
        - "--model — model identifier, stored as state['model']"
        - "workflow_name (positional) — workflow name to invoke"
      mutual_exclusion: "--prompt and --prompt-file are in an argparse mutually exclusive group"
      flow: >
        1. load_app() dynamically imports the agents file via importlib
        2. parse_state_pairs() splits key=val strings into dict
        3. If --prompt provided, sets state['prompt'] to the string value
        4. If --prompt-file provided, reads file via Path.read_text() into state['prompt']
        5. Merges --model into state dict if provided
        6. Builds CliChannel with workflow_name, initial_state=dict
        7. Creates Runner(app, channel) and calls runner.run()
        8. Wraps runner.run() in try/except for WorkflowFailedError
        9. On WorkflowFailedError: prints to stderr, exits 1
        10. On success: prints result dict to stdout
      error_handling:
        - "FileNotFoundError -> agents file not found"
        - "AttributeError -> agents file has no 'app'"
        - "FileNotFoundError -> prompt file not found (--prompt-file)"
        - "Invalid state pair (no '=') -> stderr + exit(1)"
        - "WorkflowFailedError -> stderr + exit(1)"

    server:
      args:
        - "--host (default: 127.0.0.1) — uvicorn bind host"
        - "--port (default: 8000) — uvicorn bind port"
        - "--reload (flag) — enable uvicorn auto-reload"
        - "--agents-file (default: handlers.py) — path to Python module exporting `app`"
      flow: >
        1. Sets ANTKEEPER_HANDLERS_FILE environment variable to args.agents_file
        2. Calls uvicorn.run("antkeeper.server:app", host, port, reload)
        3. Uvicorn imports antkeeper.server module and loads the app object
      notes: >
        The --agents-file is propagated to create_app() via environment variable
        because uvicorn.run() imports the module string "antkeeper.server:app" and
        the factory needs the path at import time.

    init:
      args:
        - "path (positional, optional, default: '.') — directory to create handlers.py in"
      flow: >
        1. Resolves path via os.path.realpath
        2. Checks if handlers.py already exists at target (exits 1 if so)
        3. Writes HANDLERS_TEMPLATE to target/handlers.py
        4. Prints success message with usage instructions (run, server, env vars)
      error_handling:
        - "handlers.py already exists -> stderr + exit(1)"
        - "Directory does not exist (FileNotFoundError) -> stderr + exit(1)"
        - "No write permission (PermissionError) -> stderr + exit(1)"
      template: >
        HANDLERS_TEMPLATE is a module-level string constant in cli.py that provides
        a starter handlers.py with: imports, App(), a healthcheck handler, and
        commented-out examples for workflow composition and worktree isolation.

  no_subcommand: "Prints help and exits 0"

server_module:
  file: src/antkeeper/server.py
  purpose: >
    FastAPI server orchestrator. Loads .env, loads antkeeper App, creates
    FastAPI, defines routes inline, delegates logic to http/ package.
  functions:
    create_app:
      signature: "create_app(agents_file: str = os.environ.get('ANTKEEPER_HANDLERS_FILE', 'handlers.py')) -> FastAPI"
      responsibility: >
        1. dotenv.load_dotenv()
        2. load_app(agents_file) -> antkeeper App
        3. Creates FastAPI + SlackEventProcessor(antkeeper_app)
        4. Defines @api.post("/webhook") -> delegates to handle_webhook
        5. Defines @api.post("/slack_event") -> env var guard + delegates to slack.handle_event
        6. Returns configured FastAPI
      slack_event_env_guard: >
        Before delegating to SlackEventProcessor, the /slack_event route validates
        SLACK_BOT_TOKEN and SLACK_BOT_USER_ID are present and non-empty. Raises
        HTTPException(422) listing missing vars. url_verification requests bypass
        this guard (Slack sends these during initial app setup before tokens exist).
  module_level:
    app: "app = create_app()  # Default instance for uvicorn to import"

http_modules:
  init:
    file: src/antkeeper/http/__init__.py
    function: "run_workflow_background(runner: Runner) -> None"
    behavior: >
      Shared background task helper. Catches WorkflowFailedError silently,
      logs unexpected errors to stderr. Imported by webhook.py and
      slack_events.py.

  webhook:
    file: src/antkeeper/http/webhook.py
    types:
      WebhookRequest: "pydantic.BaseModel — workflow_name: str, initial_state: dict[str, Any] = {}"
      WebhookResponse: "pydantic.BaseModel — run_id: str"
    function: "async handle_webhook(request: WebhookRequest, background_tasks: BackgroundTasks, antkeeper_app: App) -> WebhookResponse"
    behavior: >
      Validates workflow via get_handler (404 if missing), creates
      ApiChannel + Runner, schedules run_workflow_background as background
      task, returns WebhookResponse with run_id. Called from server.py
      route handler.

  slack_events:
    file: src/antkeeper/http/slack_events.py
    helpers:
      - "is_bot_message(event) -> bool  # checks event.get('bot_id')"
      - "is_bot_mention(text, bot_user_id) -> bool  # checks <@BOT_ID> in text"
      - "strip_mention(text) -> str  # regex removes leading <@U...> mention"
    types:
      PendingMessage:
        kind: dataclass
        fields: "channel_id, ts, user, text, files: list[dict], workflow_name, timer_task: asyncio.Task | None"
    async_helpers:
      slack_api: "async slack_api(token, method, payload) -> dict  # httpx.AsyncClient POST to Slack API"
    class: SlackEventProcessor
    construction: "SlackEventProcessor(antkeeper_app: App)"
    attributes:
      - "_app: App  # antkeeper app with registered handlers"
      - "_pending: dict[tuple[str, str], PendingMessage]  # instance-scoped pending store"
    methods:
      - "async handle_event(body: dict) -> dict  # main event router"
      - "async _handle_thread_reply(event, channel_id, event_ts, token, cooldown) -> dict"
      - "async _handle_edit(event, channel_id, token, cooldown) -> dict"
      - "_handle_delete(event, channel_id) -> dict  # sync method"
      - "async _handle_mention(event, channel_id, event_ts, bot_user_id, token, cooldown) -> dict"
      - "async _on_timer_fire(key, token, cooldown) -> None  # debounce timer callback"
    env_vars:
      - "SLACK_BOT_TOKEN — Slack bot OAuth token"
      - "SLACK_BOT_USER_ID — Slack user ID for mention detection"
      - "SLACK_COOLDOWN_SECONDS — debounce timer seconds (default: 30)"
    routing_order:
      - "url_verification -> return challenge"
      - "no event -> 200"
      - "bot self-filter (bot_id) -> 200"
      - "thread reply (thread_ts != ts, parent in pending) -> cancel timer, append text/files, restart timer, add reaction"
      - "message_changed -> update pending text, restart timer"
      - "message_deleted -> cancel timer, remove pending"
      - "bot mention (app_mention or message with subtype None or file_share, containing <@BOT>) -> create PendingMessage, add thumbsup reaction, start debounce timer"
      - "all other -> 200"
    debounce_timer:
      behavior: >
        _on_timer_fire: sleeps for cooldown, pops entry from pending,
        posts "Processing your request..." in thread, validates workflow exists (posts
        error if not), builds initial_state {prompt, slack_user, files?},
        creates SlackChannel + Runner, runs via
        asyncio.to_thread(run_workflow_background, runner).
    design_constraint: >
      Pending store is instance-scoped on SlackEventProcessor for test
      isolation. Cooldown=0 in tests for deterministic timers. No thread
      safety concerns — pending accessed only from async handlers on
      single-threaded event loop. (channel_id, ts) key naturally
      deduplicates app_mention + message events.

handlers_file:
  file: handlers.py
  purpose: >
    Default user-land agents file exporting `app`. Contains LLM-backed
    workflow handlers that use ClaudeCodeAgent to run slash commands,
    extract structured JSON from responses, and thread results through state.
  app: "App()  # default log_dir and worktree_dir"
  code_organization: >
    File is structured in three sections, marked by comment headers:
    1. Steps (# --- Steps ---) — individual handler functions
    2. Shared workflow constants (# --- Shared workflow constants ---) —
       step lists shared by multiple workflows (e.g. SDLC_STEPS).
       Constants are only defined when shared; otherwise steps are inlined.
    3. Workflows (# --- Workflows ---) — composite handlers that call
       run_workflow or orchestrate steps.
  steps:
    healthcheck:
      description: "Asks Claude to write a short poem; verifies the agent pipeline works"
      state_in: ["model"]
      state_out: ["poem"]

    specify:
      description: "Runs /specify, extracts spec_file and slug via extract_json"
      state_in: ["prompt", "model"]
      state_out: ["spec_file", "slug"]

    branch:
      description: "Runs /branch, extracts branch_name via extract_json"
      state_in: ["spec_file", "model"]
      state_out: ["branch_name"]

    implement:
      description: "Runs /implement against spec_file"
      state_in: ["spec_file", "model"]
      state_out: ["implement_status"]

    document:
      description: "Runs /document to update docs for current branch"
      state_in: ["model"]
      state_out: ["document_status"]

    derive_feature:
      description: "Runs /derive_feature, extracts feature_type and slug via extract_json"
      state_in: ["prompt", "model"]
      state_out: ["feature_type", "slug"]

  shared_constants:
    SDLC_STEPS: "[specify, branch, implement, document]  # used by sdlc and specify_and_branch"

  workflows:
    sdlc:
      description: "Composite workflow: specify -> branch -> implement -> document"
      steps: "run_workflow(runner, state, SDLC_STEPS)"

    specify_and_branch:
      description: "Partial SDLC: specify -> branch only"
      steps: "run_workflow(runner, state, SDLC_STEPS[0:2])"

    sdlc_iso:
      description: >
        Isolated SDLC workflow. Calls derive_feature to get feature_type
        and slug, creates a git worktree named {timestamp}-{run_id} on a
        branch {feature_type}/{slug}, runs specify -> implement -> document
        inside the worktree, then returns. Worktree is NOT removed on exit
        (remove=False). Steps are inlined as [specify, implement, document]
        (no shared constant — only used by this workflow).
      state_in: ["prompt", "model"]
      state_out: ["feature_type", "slug", "spec_file", "implement_status", "document_status", "worktree_path", "branch_name"]

data_flow:
  cli_happy_path:
    - "CLI parses args -> loads agents file -> gets App"
    - "Builds CliChannel(workflow_name, initial_state)"
    - "Runner(app, channel).run() — creates log file, generates run id"
    - "Runner merges initial_state + {run_id, workflow_name}"
    - "Handler(runner, state) -> new State"
    - "For composites: run_workflow folds state through sub-handlers"
    - "Result state printed to stdout"
  api_happy_path:
    - "POST /webhook with {workflow_name, initial_state} JSON body"
    - "Server validates workflow exists via app.get_handler()"
    - "Creates ApiChannel(workflow_name, initial_state)"
    - "Creates Runner(app, channel) — generates run_id"
    - "Schedules run_workflow_background(runner) as FastAPI background task"
    - "Returns {run_id} JSON response immediately (200)"
    - "Background task calls runner.run() -> workflow executes asynchronously"
    - "Progress/error output goes to Uvicorn logs (stdout/stderr)"
  llm_path:
    - "Handler extracts prompt and model from state"
    - "Creates ClaudeCodeAgent(model=state.get('model'))"
    - "Calls agent.prompt('/<command> <prompt>')"
    - "Agent runs subprocess `claude -p` -> stdout returned"
    - "Handler spreads state + {result: response}"
  worktree_iso_path:
    - "sdlc_iso calls derive_feature to get feature_type and slug"
    - "Creates Worktree(base_dir=runner.app.worktree_dir, name='{timestamp}-{run_id}')"
    - "Enters git_worktree(wt, create=True, branch='{feature_type}/{slug}', remove=False)"
    - "Inside worktree: runs specify -> implement -> document via run_workflow"
    - "On exit: cwd restored, worktree kept on disk for inspection"
    - "Returns state with worktree_path and branch_name added"
  slack_happy_path:
    - "User @mentions bot in Slack channel with workflow name"
    - "POST /slack_event validates SLACK_BOT_TOKEN + SLACK_BOT_USER_ID (422 if missing, url_verification exempt)"
    - "POST /slack_event receives event_callback, bot filters + mention detection"
    - "Creates PendingMessage, adds thumbsup reaction, starts debounce timer"
    - "User can edit message or reply in thread — timer resets, text/files accumulated"
    - "Timer fires after cooldown — posts 'Processing your request...' in thread"
    - "Validates workflow exists, builds initial_state {prompt, slack_user, files?}"
    - "Creates SlackChannel(workflow_name, initial_state, token, channel_id, thread_ts)"
    - "Creates Runner(app, channel), runs via asyncio.to_thread(run_workflow_background)"
    - "Handler calls runner.report_progress() -> SlackChannel posts to Slack thread"
  error_path:
    - "Unknown handler -> ValueError from App.get_handler"
    - "Handler calls runner.fail(msg) -> logs, raises WorkflowFailedError"
    - "CLI catches WorkflowFailedError -> prints to stderr, exit(1)"
    - "API catches WorkflowFailedError -> logs only (already logged by Runner), continues serving"
    - "Handler calls runner.report_error(msg) -> logs, channel.report_error -> stderr"
    - "Agent subprocess failure -> AgentExecutionError propagates"
    - "Workflow exception -> Runner logs error, re-raises"
    - "Worktree creation failure -> WorktreeError propagates"
    - "Entering non-existent worktree (create=False) -> WorktreeError"
    - "API unknown workflow -> HTTPException 404 before creating Runner"
    - "API invalid request body -> FastAPI auto-returns 422"
    - "Slack missing env vars -> HTTPException 422 from route handler (url_verification exempt)"

testing_design:
  philosophy: >
    Test the framework, not app logic. Each test owns its setup. Replace
    I/O at the channel boundary with capturing doubles.
  test_channel:
    file: tests/conftest.py
    class: TestChannel
    satisfies: Channel protocol (duck-typed)
    captures:
      - "progress_messages: list[str]"
      - "error_messages: list[str]"
  fixtures:
    app:
      signature: "() -> App"
      notes: "Creates App with log_dir, worktree_dir, and state_dir each set to separate tempfile.mkdtemp() directories"
    runner_factory:
      signature: "(test_app=None, workflow_name='test', initial_state=None) -> (Runner, TestChannel)"
      notes: "Uses injected app fixture by default; test_app overrides if provided"
    git_repo:
      file: tests/git/conftest.py
      signature: "() -> str (yields repo path)"
      notes: >
        Creates a temporary git repo with init commit, sets cwd to repo dir.
        Restores original cwd on teardown. Used by all git integration tests.
  test_structure: >
    Tests organized into subdirectories mirroring src layout.
  test_files:
    - tests/core/test_workflows.py — core engine paths (single, multi-step, failure raises WorkflowFailedError, unknown)
    - tests/core/test_logging.py — per-run file logging, App config (log_dir, worktree_dir, state_dir defaults)
    - tests/core/test_state_persistence.py — state file creation, naming format, content, per-step persistence, log/state filename correlation
    - tests/channels/test_cli_channel.py — CliChannel initial_state and attribute tests
    - tests/channels/test_api_channel.py — ApiChannel type, initial_state, stdout/stderr reporting
    - tests/test_cli.py — CLI arg parsing, mutual exclusion, prompt-file loading, WorkflowFailedError handling, integration
    - tests/test_server.py — FastAPI webhook endpoint (returns run_id, 404 for unknown workflow, 422 for invalid body)
    - tests/channels/test_slack_channel.py — SlackChannel type, initial_state, report_progress/error posts, HTTP failure resilience
    - tests/test_slack_server.py — Slack event endpoint (url_verification, bot filter, mention, edit, reply, delete, timer dispatch, unknown workflow, dedup, env var validation)
    - tests/llm/test_claude_code_agent.py — ClaudeCodeAgent subprocess and error handling
    - tests/helpers/test_extract_json.py — JSON extraction from LLM responses
    - tests/git/test_worktree.py — Worktree class (path, exists, create, create+branch, remove, errors)
    - tests/git/test_context.py — git_worktree context manager (create, cwd, yield, restore, remove)

key_design_decisions:
  - >
    Channel is a Protocol, not an ABC. Extensions use structural subtyping.
  - >
    Handlers receive (runner, state) and return State. Runner is the only
    framework object a handler interacts with.
  - >
    State is always a new dict (spread pattern). No mutation of incoming state.
    State is automatically persisted to JSON files during workflow execution
    (before, after each step, and on completion) but is never shared between
    workflow runs — each run writes its own isolated state file.
  - >
    run_workflow enables composition without inheritance or a DAG scheduler.
    Handlers that orchestrate other handlers just call run_workflow with a
    step list.
  - >
    The CLI dynamically loads user code via importlib. The agents file must
    export a module-level `app` object.
  - >
    Agent is a Protocol, like Channel. New LLM backends implement prompt(str)->str.
    ClaudeCodeAgent delegates to the `claude` CLI via subprocess.
  - >
    CLI passes --prompt (or --prompt-file) and --model into state dict.
    --prompt and --prompt-file are mutually exclusive; file contents are
    resolved in cli.py before reaching the channel. Handlers read from
    state, keeping the Runner/Channel layer unaware of LLM concerns.
  - >
    Errors propagate by default. Handler errors (missing keys), runtime issues
    (failed connections), and coding errors (naming mistakes) all propagate.
    Only edge validations (e.g. missing CLI params) are caught and reported
    when a stack trace would add no value.
  - >
    Per-run file logging. Each Runner creates an isolated log file. No console
    logging from the framework. Module-level loggers provide debug tracing.
  - >
    Git worktree isolation. The sdlc_iso handler creates a git worktree for
    each run, branching on {feature_type}/{slug}. This enables parallel
    feature development without cwd or branch conflicts. Worktrees are kept
    after workflow completion (remove=False) so developers can inspect results.
    App.worktree_dir configures the base directory (default "trees/").
  - >
    BREAKING: Runner.fail() raises WorkflowFailedError instead of exit(1).
    This decouples workflow failure from process termination, enabling both
    CLI (which exits) and API (which continues serving) to handle failures
    appropriately. CLI catches WorkflowFailedError and exits 1. API server
    catches it in background task and continues serving other requests.
  - >
    Dual execution model: CLI and HTTP API. CliChannel and ApiChannel are
    nearly identical (both print to stdout/stderr) but have different type
    identifiers. The server module provides POST /webhook for async workflow
    execution. FastAPI BackgroundTasks runs workflows in same process after
    response — acceptable for initial implementation, production can scale
    with multiple uvicorn workers.
  - >
    Server factory pattern: create_app() loads agents file and returns
    configured FastAPI instance. This makes testing possible — tests call
    create_app() with custom agents file. Module-level app = create_app()
    provides default instance for uvicorn to import. The --agents-file CLI
    option is propagated via ANTKEEPER_HANDLERS_FILE environment variable.
  - >
    Server as thin orchestrator. server.py defines routes inline and
    delegates to http/ package: handle_webhook (webhook.py) and
    SlackEventProcessor.handle_event (slack_events.py). Background
    task helper run_workflow_background lives in http/__init__.py.
    No setup_*_routes functions — server owns route definitions.
  - >
    Slack integration uses debounce pattern via SlackEventProcessor class
    (not closures). Bot mentions create a PendingMessage with a cooldown
    timer. Edits and thread replies reset the timer and accumulate
    text/files. Timer fire dispatches to the workflow handler. Pending
    store is instance-scoped on the processor for test isolation.
  - >
    SlackChannel uses sync httpx (handlers run in threadpool). Server
    endpoint uses async httpx (event loop). Independent clients.
    Channel captures thread context (channel_id, thread_ts) at construction
    from the original @mention event.
  - >
    Slack env vars (SLACK_BOT_TOKEN, SLACK_BOT_USER_ID, SLACK_COOLDOWN_SECONDS)
    read from environment at request time. python-dotenv loads .env file in
    create_app(). .env is gitignored.
  - >
    Automatic state persistence. Runner persists state as JSON to
    <state_dir>/<timestamp>-<id>.json at key lifecycle points: before
    workflow, after each run_workflow step, and after completion. State
    and log files share the same <timestamp>-<id> base name for
    correlation. App.state_dir configures the directory (default
    ".antkeeper/state/"). This enables debugging and mid-workflow recovery
    without handlers needing explicit persistence logic.
